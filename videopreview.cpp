#include "videopreview.h"
#include <QDebug>
#include <stdint.h>

VideoPreviewThread tVideoPreview;

int initvideo()
{
    //avcodec_init();
    av_register_all();
    printf("init\n");
    return 0;
}
int i=initvideo();

QImage *SaveFrame(AVFrame *pFrame, int width, int height, int iFrame)
{
    int linesize=pFrame->linesize[0];
    if(width<=0) return NULL;
    if(height<=0) return NULL;
    if(linesize<=0) return NULL;
    if(!pFrame->data[0]) return NULL;
    uint32_t *imgdata=(uint32_t *)malloc(width*height*4);
      memset(imgdata,'\0',width*height*4);

      for (int i=0; i<height; i++)
        {
          for (int j=0; j<width; j++)
        {
          int offset=i*pFrame->linesize[0]+3*j;
          unsigned char *pixel=pFrame->data[0]+offset;
          imgdata[i*width+j]=(pixel[0]<<16) | (pixel[1]<<8) | pixel[2];
        }
        }


    //qDebug()<<pFrame->data[0];
    //qDebug()<<width<<" "<<height<<" "<<linesize;

    QImage *ret=new QImage((uchar *)imgdata,width,height,width*4,QImage::Format_RGB32);
    //if(imgdata) free(imgdata);  absturtz bei free
    return ret;
}
QMap<QString,QImage *> framecache;
void clearVideoFrameCache()
{
//#warning free images
    framecache.clear();
}

QImage * getVideoFrameCached(const char *filename, double t)
{
    if(framecache.contains(filename))
        return framecache.value(filename);
    tVideoPreview.addPreview(filename,t);
    //QImage *ret=getVideoFrame(filename,t);
    //framecache[filename]=ret;
    return NULL;
}

QImage * getVideoFrame(const char *filename, double t)
{
#ifdef VIDEOPREVIEW
    printf("getvideosnapshot(%s,%f)\n",filename,t);
    AVFormatContext *pFormatCtx=NULL;
    int             i=0, videoStream=0;
    AVCodecContext  *pCodecCtx=NULL;
    AVCodec         *pCodec=NULL;
    AVFrame         *pFrame=NULL;
    AVFrame         *pFrameRGB=NULL;
    AVPacket        packet;
    int             frameFinished;
    int             numBytes;
    uint8_t         *buffer=NULL;
int rc=0;
    QImage *ret=NULL;


    // Open video file
    if((rc=avformat_open_input(&pFormatCtx, filename, NULL, 0))!=0)
    {
        qDebug()<<"couldnt open video file "<<filename<<" rc="<<rc;
        return NULL; // Couldn't open file
    }

    // Retrieve stream information
    if(avformat_find_stream_info(pFormatCtx,NULL)<0)
        return NULL; // Couldn't find stream information

    // Dump information about file onto standard error
    //dump_format(pFormatCtx, 0, filename, false);

    // Find the first video stream
    videoStream=-1;
    for(i=0; i<(int)pFormatCtx->nb_streams; i++)
        if(pFormatCtx->streams[i]->codec->codec_type==AVMEDIA_TYPE_VIDEO)
        {
            videoStream=i;
            break;
        }
    if(videoStream==-1)
        return NULL; // Didn't find a video stream

    // Get a pointer to the codec context for the video stream
    pCodecCtx=pFormatCtx->streams[videoStream]->codec;

    // Find the decoder for the video stream
    //avmutex.lock();
    pCodec=avcodec_find_decoder(pCodecCtx->codec_id);
    //avmutex.unlock();
    if(pCodec==NULL)
        return NULL; // Codec not found

    // Open codec
    //avmutex.lock();
    rc=avcodec_open2(pCodecCtx, pCodec,NULL);
    //avmutex.unlock();
    if(rc<0)
        return NULL; // Could not open codec

    // Hack to correct wrong frame rates that seem to be generated by some codecs
    if(pCodecCtx->time_base.num>1000 && pCodecCtx->time_base.den==1)
        pCodecCtx->time_base.den=1000;

    // Allocate video frame
    pFrame=av_frame_alloc();

    // Allocate an AVFrame structure
    pFrameRGB=av_frame_alloc();
    if(pFrameRGB==NULL)
        return NULL;

    // Determine required buffer size and allocate buffer
    numBytes=avpicture_get_size(AV_PIX_FMT_RGB24, pCodecCtx->width,
                                pCodecCtx->height);

    buffer=(uint8_t *)malloc(numBytes);

    // Assign appropriate parts of buffer to image planes in pFrameRGB
    avpicture_fill((AVPicture *)pFrameRGB, buffer, AV_PIX_FMT_RGB24,
                   pCodecCtx->width, pCodecCtx->height);

    //printf("AV_TIME_BASE:%li\n",(long)AV_TIME_BASE);
    //printf("avseek:%li %f\n",(long)static_cast<int64_t>(t*AV_TIME_BASE+0.5),t);
    av_seek_frame(pFormatCtx,-1,static_cast<int64_t>(t*AV_TIME_BASE+0.5),AVSEEK_FLAG_ANY);
    av_seek_frame(pFormatCtx, -1, static_cast<int64_t>(t*AV_TIME_BASE+0.5), AVSEEK_FLAG_BACKWARD);
    avcodec_flush_buffers(pCodecCtx);
    // Read frames and save first five frames to disk
    i=0;
    int found_frame=0;
    double frame_rate = av_q2d(pFormatCtx->streams[videoStream]->r_frame_rate);
    int64_t targetPts = static_cast<int64_t>(t * AV_TIME_BASE + 0.5);

    while((av_read_frame(pFormatCtx, &packet)>=0)&&(!found_frame))
    {
        // Is this a packet from the video stream?
        if(packet.stream_index==videoStream)
        {
            // Decode video frame
            avcodec_decode_video2(pCodecCtx, pFrame, &frameFinished,
                                  &packet);

            // Did we get a video frame?
            if(frameFinished)
            {
                int64_t pts=0;
                if(packet.duration&&frame_rate)
                    pts = static_cast<int64_t>(packet.pts / packet.duration * AV_TIME_BASE / frame_rate + 0.5);
                //printf("frameno:%li %li\n",(long)pts,(long)targetPts);
                if(pts>=targetPts)
                {
                    found_frame=1;
                     struct SwsContext *img_convert_ctx=NULL;

                    // Convert the image into YUV format that SDL uses
                    if(img_convert_ctx == NULL) {
                        int w = pCodecCtx->width;
                        int h = pCodecCtx->height;

                        img_convert_ctx = sws_getContext(w, h,
                                                         pCodecCtx->pix_fmt,
                                                         w, h, AV_PIX_FMT_BGR24, SWS_BICUBIC,
                                                         NULL, NULL, NULL);
                        if(img_convert_ctx == NULL) {
                            fprintf(stderr, "Cannot initialize the conversion context!\n");
                            return NULL;
                        }
                    }
                    int rc = sws_scale(img_convert_ctx, pFrame->data, pFrame->linesize, 0,
                                       pCodecCtx->height, pFrameRGB->data, pFrameRGB->linesize);

                    qDebug()<<"get frame from "<<filename;
                    ret=SaveFrame(pFrameRGB, pCodecCtx->width, pCodecCtx->height, i);
                }
                i++;
            }
        }

        // Free the packet that was allocated by av_read_frame
        av_free_packet(&packet);
    }

    // Free the RGB image
    free(buffer);
    av_free(pFrameRGB);

    // Free the YUV frame
    av_free(pFrame);

    // Close the codec
    avcodec_close(pCodecCtx);

    // Close the video file
    avformat_close_input(&pFormatCtx);

    return ret;
#else
    return NULL;
#endif
}

VideoPreviewThread::VideoPreviewThread(QObject *parent)
{
    running=false;
}

void VideoPreviewThread::run() {
    running=true;

    bool done=false;
    while (!done)
    {
        if(!do_filename.size()) done=true;

        if(!done)
        {
            QString filename;
            double t;
            filename=do_filename.front();
            t=do_t.front();
            QImage *ret=getVideoFrame(filename.toLocal8Bit().data(),t);
                framecache[filename]=ret;
                do_filename.pop_front();
                do_t.pop_front();
                emit videoPreviewUpdate();
        }

    }
    running=false;

}
void VideoPreviewThread::addPreview(QString const &file,double t)
{
    do_filename.push_back(file);
    do_t.push_back(t);
    if(!running)
        start();
}
